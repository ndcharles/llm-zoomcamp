{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026f1dbe",
   "metadata": {},
   "source": [
    "## Vector Search\n",
    "\n",
    "Traditional keyword search works by matching exact words. This works well when you know the precise keywords present in the data. But what happens when there are no keywords? What if you're searching through images, audio, video or code, or even cross-modally? Vector search retrieves information based on semantic similarity measured numerically between vectorized data representations (embeddings). It recognizes patterns and relationships between concepts, enabling search systems to retrieve the most relevant content, even when the phrasing differs, terminology varies, or no explicit keywords exist.\n",
    "\n",
    "While traditional databases that serve as data stores, vector databases are more like search engines. They are designed to be scalable, always available, and capable of delivering high-speed search results even under heavy loads. Just as Google or Bing can handle billions of queries at once, vector databases are designed for scenarios where rapid, high-throughput, low-latency retrieval is a must.\n",
    "\n",
    "### Vectors\n",
    "Vectors (also known as embeddings) are high-dimensional representations of various data points — texts, images, videos, etc. Many state-of-the-art (SOTA) embedding models generate representations of over 1,500 dimensions. When it comes to state-of-the-art PDF retrieval, the representations can reach over 100,000 dimensions per page.\n",
    "\n",
    "#### Properties of vectors\n",
    "- Vectors are heavy\n",
    "- They are obtained from some other source-of-truth data. (They are always a transformation of other data (text, video, image, audio))\n",
    "- They are fixed-size\n",
    "- The same embedding model should be used to maintain the geometry of the vector space\n",
    "- To enjoy the benefits of vector data, we need to store it separately\n",
    "\n",
    "Vector search relies on high-dimensional vector mathematics, making it computationally heavy at scale. A brute-force similarity search would require comparing a query against every vector in the database which is unfeasible for production scenarios where a db can have up to 100 million records.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) and agentic RAG use vector databases as a knowledge source to retrieve context for large language models (LLMs). In the first part, vector search allows us to refine our choices based on similarity and dissimilarity rather than starting with a fixed query. This flexibility is possible because vector search is not tied to the binary “match/not match” concept but operates on distances in a vector space.\n",
    "\n",
    "### Quadrant\n",
    "Qdrant is an open-source vector search engine, a dedicated solution built in Rust for scalable vector search.\n",
    "\n",
    "- To make production-level vector search at scale;\n",
    "- To stay in sync with the latest trends and best practices;\n",
    "- To fully use vector search capabilities (including those beyond simple similarity search).\n",
    "\n",
    "#### Installing Quadrant\n",
    "\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant\n",
    "```\n",
    "\n",
    "The second line in the docker run command mounts local storage to keep your data persistent. So even if you restart or delete the container, your data will still be stored locally.\n",
    "\n",
    "- 6333 – REST API port\n",
    "- 6334 – gRPC API port\n",
    "\n",
    "Qdrant provides a built-in Web UI you can use it to inspect collections, check system health, and even run simple queries.\n",
    "When you're running Qdrant in Docker, the Web UI is available at http://localhost:6333/dashboard\n",
    "\n",
    "#### Installing Required Libraries\n",
    "In the environment created specifically for this course, we’ll install:\n",
    "\n",
    "- The ```qdrant-client package```. Qdrant offers official clients for Python, JavaScript/TypeScript, Go, and Rust.\n",
    "- The ```fastembed package``` - an optimized embedding (data vectorization) solution designed specifically for Qdrant. Make sure you install version ```>= 1.14.2``` to use the local inference with Qdrant.\n",
    "\n",
    "### References\n",
    "- [vector search](https://qdrant.tech/articles/dedicated-vector-search/)\n",
    "- [search](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/02-vector-search/sematic_search.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ebd1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script humanfriendly is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts huggingface-cli and tiny-agents are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script coloredlogs is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script onnxruntime_test is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe16fd",
   "metadata": {},
   "source": [
    "I am on codespaces so my port is forwarded to https://super-duper-doodle-55v9gv4wrv92v764-6333.app.github.dev/dashboard just add /dashboard to the forwarded port URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e75951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from qdrant_client import QdrantClient, models\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f993ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the client and connect to our local instance\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d0465",
   "metadata": {},
   "source": [
    "### Step 2: Study the Dataset\n",
    "To build a working vector search solution (and, more generally, to understand if/when/how it’s needed), it's good to study the dataset and figure out the nature and structure of the data we’re working with, for example:\n",
    "\n",
    "- modality — is it text, images, videos, a combination?\n",
    "- specifics — if it’s text: language used, how big are the text pieces, are there any special characters, etc.\n",
    "\n",
    "It will help us define:\n",
    "\n",
    "- the right data \"schema\" (what to vectorize, what to store as metadata, etc);\n",
    "- the right embedding model (the best fit based on the domain, precision & resource requirements).\n",
    "\n",
    "We have a toy dataset provided for experimentation, let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fc0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35250b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9a2da",
   "metadata": {},
   "source": [
    "Data already seems cleaned and chunked (i.e., divided into small pieces that embedding models can easily digest), so what's left is to define:\n",
    "\n",
    "- which fields could be used for semantic search ;\n",
    "- which fields should be stored as metadata, e.g. useable for filtering conditions;\n",
    "\n",
    "We have a dataset with three course types:\n",
    "```data-engineering-zoomcamp```, ```machine-learning-zoomcamp```, and ```mlops-zoomcamp```.\n",
    "Each course includes a collection of ```question``` and `text (answer)` pairs, along with the `section` the question refers to.\n",
    "\n",
    "From the earlier step, we will store the `course` and `section` fields as metadata.\n",
    "This way, we can filter search results when asking questions related to a specific course or a specific section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c519d6b",
   "metadata": {},
   "source": [
    "### Step 3: Choosing the Embedding Model with FastEmbed\n",
    "Now that we know we're embedding small chunks of English text (course-related question and answer pairs), we can choose a suitable embedding model to convert this data into vectors.\n",
    "\n",
    "The choice of an embedding model depends on many factors:\n",
    "\n",
    "- The task, data modality, and data specifics;\n",
    "- The trade-off between search precision and resource usage (larger embeddings require more storage and memory);\n",
    "- The cost of inference (especially if you're using a third-party provider);\n",
    "etc\n",
    "\n",
    "---\n",
    "\n",
    "[FastEmbed](https://github.com/qdrant/fastembed) is an optimized embedding solution designed specifically for Qdrant. It delivers low-latency, CPU-friendly embedding generation, eliminating the need for heavy frameworks like PyTorch or TensorFlow. It uses quantized model weights and ONNX Runtime, making it significantly faster than traditional Sentence Transformers on CPU while maintaining competitive accuracy.\n",
    "\n",
    "FastEmbed supports:\n",
    "\n",
    "- Dense embeddings for text and images (the most common type in vector search, ones we're going to use today)\n",
    "- Sparse embeddings (e.g., BM25 and sparse neural embeddings)\n",
    "- Multivector embeddings (e.g., ColPali and ColBERT, late interaction models)\n",
    "- Rerankers\n",
    "\n",
    "All of these can be directly used in Qdrant (as Qdrant supports dense, sparse & multivectors along with hybrid search).\n",
    "FastEmbed’s integration with Qdrant allows you to directly pass text or images to the Qdrant client for embedding.\n",
    "\n",
    "### FastEmbed for Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n",
    "TextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b066078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an embedding model suitable for English text.\n",
    "# It also makes sense to select a unimodal model, since we’re not including images in our search\n",
    "# Also model that produces small-to-moderate-sized embeddings (e.g., 512 dimensions), so we don’t overuse resources in our simple setup. \n",
    "\n",
    "import json\n",
    "\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model[\"dim\"] == EMBEDDING_DIMENSIONALITY:\n",
    "        print(json.dumps(model, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c237f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jina-embeddings-v2-small-en has the best dim and match our preference\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee5197",
   "metadata": {},
   "source": [
    "Now we’re ready to configure and use Qdrant for semantic search. To fully understand what’s happening, here’s a quick overview of Qdrant’s core terminology:\n",
    "\n",
    "- Points are the central entity Qdrant works with. A point is a record consisting of an ID, a vector, and an optional payload.\n",
    "- A collection is a named set of points (i.e., vectors with optional payloads) that you can search within. Think of it as the container for your vector search solution, a single business problem solved.A collection is similar to index we used in elastic search ??\n",
    "\n",
    "Qdrant supports different types of vectors to enable different modes of data exploration and search (dense, sparse, multivectors, and named vectors). In this example, we’ll use the most common type, dense vectors.\n",
    "\n",
    "> Embeddings capture the semantic essence of the data, while the payload holds structured metadata.\n",
    "> This metadata becomes especially useful when applying filters or sorting during search. Qdrant's payloads can hold structured data like booleans, keywords, geo-locations, arrays, and nested objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b82c10",
   "metadata": {},
   "source": [
    "### Step 4: Create a Collection\n",
    "When creating a collection, we need to specify:\n",
    "\n",
    "- Name: A unique identifier for the collection.\n",
    "- Vector Configuration:\n",
    "  - Size: The dimensionality of the vectors.\n",
    "  - Distance Metric: The method used to measure similarity between vectors.\n",
    "\n",
    "There are additional parameters you can explore in our [documentationtext](https://qdrant.tech/documentation/concepts/collections/#create-a-collection)]. Moreover, you can configure other vector types in Qdrant beyond typical dense embeddings (e.g., for hybrid search). However, for this example, the simplest default configuration is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d7f0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the collection name\n",
    "collection_name = \"zoomcamp-rag\"\n",
    "\n",
    "# Create the collection with specified vector parameters\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
    "        distance=models.Distance.COSINE  # Distance metric for similarity search\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23993b10",
   "metadata": {},
   "source": [
    "### Step 5: Create, Embed & Insert Points into the Collection\n",
    "\n",
    "[Points](https://qdrant.tech/documentation/concepts/points/#points) are the core data entities in Qdrant. Each point consists of:\n",
    "\n",
    "- ID. A unique identifier. Qdrant supports both 64-bit unsigned integers and UUIDs.\n",
    "- Vector. The embedding that represents the data point in vector space.\n",
    "- Payload (optional). Additional metadata as key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "454f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "id = 0\n",
    "\n",
    "for course in documents_raw:\n",
    "    for doc in course['documents']:\n",
    "\n",
    "        point = models.PointStruct(\n",
    "            id=id,\n",
    "            vector=models.Document(text=doc['text'], model=model_handle),\n",
    "            payload={\n",
    "                \"text\": doc['text'],\n",
    "                \"section\": doc['section'],\n",
    "                \"course\": course['course']\n",
    "            } #save all needed metadata fields\n",
    "        )\n",
    "        points.append(point)\n",
    "\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64649d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41eb4fc646c948b4ac2761e2c91e0f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fadf0934b754571b4e6af9e77cdf497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a181954fd544105a488a68d1528b342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a1d8a549794e9784971ab52b9fb992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb0d4a82b124469a5390d3c96f9358b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6bbfc1a795499a9cc5572fadac63a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed and upload points to our collection.\n",
    "# First, FastEmbed will fetch&download the selected model (path defaults to os.path.join(tempfile.gettempdir(), \"fastembed_cache\")), and perform inference directly on your machine.\n",
    "# Then, the generated points will be upserted into the collection, and the vector index will be built.\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c766dad",
   "metadata": {},
   "source": [
    "In addition to basic upsert, Qdrant supports batch upsert in both column- and record-oriented formats.\n",
    "\n",
    "The Python client offers:\n",
    "- Parallelization\n",
    "- Retries\n",
    "- Lazy batching\n",
    "These can be configured via parameters in the upload_collection and upload_points functions.\n",
    "For details, check the [documentation](https://qdrant.tech/documentation/concepts/points/#upload-points)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
