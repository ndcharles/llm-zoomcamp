{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026f1dbe",
   "metadata": {},
   "source": [
    "## Vector Search\n",
    "\n",
    "Traditional keyword search works by matching exact words. This works well when you know the precise keywords present in the data. But what happens when there are no keywords? What if you're searching through images, audio, video or code, or even cross-modally? Vector search retrieves information based on semantic similarity measured numerically between vectorized data representations (embeddings). It recognizes patterns and relationships between concepts, enabling search systems to retrieve the most relevant content, even when the phrasing differs, terminology varies, or no explicit keywords exist.\n",
    "\n",
    "> Keywords search is precise while vector search (due to its embeddings) is flexible.\n",
    "\n",
    "While traditional databases that serve as data stores, vector databases are more like search engines. They are designed to be scalable, always available, and capable of delivering high-speed search results even under heavy loads. Just as Google or Bing can handle billions of queries at once, vector databases are designed for scenarios where rapid, high-throughput, low-latency retrieval is a must.\n",
    "\n",
    "### Vectors\n",
    "Vectors (also known as embeddings) are high-dimensional representations of various data points — texts, images, videos, etc. Many state-of-the-art (SOTA) embedding models generate representations of over 1,500 dimensions. When it comes to state-of-the-art PDF retrieval, the representations can reach over 100,000 dimensions per page.\n",
    "\n",
    "#### Properties of vectors\n",
    "- Vectors are heavy\n",
    "- They are obtained from some other source-of-truth data. (They are always a transformation of other data (text, video, image, audio))\n",
    "- They are fixed-size\n",
    "- The same embedding model should be used to maintain the geometry of the vector space\n",
    "- To enjoy the benefits of vector data, we need to store it separately\n",
    "\n",
    "Vector search relies on high-dimensional vector mathematics, making it computationally heavy at scale. A brute-force similarity search would require comparing a query against every vector in the database which is unfeasible for production scenarios where a db can have up to 100 million records.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) and agentic RAG use vector databases as a knowledge source to retrieve context for large language models (LLMs). In the first part, vector search allows us to refine our choices based on similarity and dissimilarity rather than starting with a fixed query. This flexibility is possible because vector search is not tied to the binary “match/not match” concept but operates on distances in a vector space.\n",
    "\n",
    "### Quadrant\n",
    "Qdrant is an open-source vector search engine, a dedicated solution built in Rust for scalable vector search.\n",
    "\n",
    "- To make production-level vector search at scale;\n",
    "- To stay in sync with the latest trends and best practices;\n",
    "- To fully use vector search capabilities (including those beyond simple similarity search).\n",
    "\n",
    "#### Installing Quadrant\n",
    "\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant\n",
    "```\n",
    "\n",
    "The second line in the docker run command mounts local storage to keep your data persistent. So even if you restart or delete the container, your data will still be stored locally.\n",
    "\n",
    "- 6333 – REST API port\n",
    "- 6334 – gRPC API port\n",
    "\n",
    "Qdrant provides a built-in Web UI you can use it to inspect collections, check system health, and even run simple queries.\n",
    "When you're running Qdrant in Docker, the Web UI is available at http://localhost:6333/dashboard\n",
    "\n",
    "#### Installing Required Libraries\n",
    "In the environment created specifically for this course, we’ll install:\n",
    "\n",
    "- The ```qdrant-client package```. Qdrant offers official clients for Python, JavaScript/TypeScript, Go, and Rust.\n",
    "- The ```fastembed package``` - an optimized embedding (data vectorization) solution designed specifically for Qdrant. Make sure you install version ```>= 1.14.2``` to use the local inference with Qdrant.\n",
    "\n",
    "### References\n",
    "- [vector search](https://qdrant.tech/articles/dedicated-vector-search/)\n",
    "- [search](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/02-vector-search/sematic_search.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ebd1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script humanfriendly is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts huggingface-cli and tiny-agents are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script coloredlogs is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script onnxruntime_test is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe16fd",
   "metadata": {},
   "source": [
    "I am on codespaces so my port is forwarded to https://super-duper-doodle-55v9gv4wrv92v764-6333.app.github.dev/dashboard just add /dashboard to the forwarded port URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e75951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from qdrant_client import QdrantClient, models\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f993ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the client and connect to our local instance\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d0465",
   "metadata": {},
   "source": [
    "### Step 2: Study the Dataset\n",
    "To build a working vector search solution (and, more generally, to understand if/when/how it’s needed), it's good to study the dataset and figure out the nature and structure of the data we’re working with, for example:\n",
    "\n",
    "- modality — is it text, images, videos, a combination?\n",
    "- specifics — if it’s text: language used, how big are the text pieces, are there any special characters, etc.\n",
    "\n",
    "It will help us define:\n",
    "\n",
    "- the right data \"schema\" (what to vectorize, what to store as metadata, etc);\n",
    "- the right embedding model (the best fit based on the domain, precision & resource requirements).\n",
    "\n",
    "We have a toy dataset provided for experimentation, let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fc0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35250b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9a2da",
   "metadata": {},
   "source": [
    "Data already seems cleaned and chunked (i.e., divided into small pieces that embedding models can easily digest), so what's left is to define:\n",
    "\n",
    "- which fields could be used for semantic search ;\n",
    "- which fields should be stored as metadata, e.g. useable for filtering conditions;\n",
    "\n",
    "We have a dataset with three course types:\n",
    "```data-engineering-zoomcamp```, ```machine-learning-zoomcamp```, and ```mlops-zoomcamp```.\n",
    "Each course includes a collection of ```question``` and `text (answer)` pairs, along with the `section` the question refers to.\n",
    "\n",
    "From the earlier step, we will store the `course` and `section` fields as metadata.\n",
    "This way, we can filter search results when asking questions related to a specific course or a specific section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c519d6b",
   "metadata": {},
   "source": [
    "### Step 3: Choosing the Embedding Model with FastEmbed\n",
    "Now that we know we're embedding small chunks of English text (course-related question and answer pairs), we can choose a suitable embedding model to convert this data into vectors.\n",
    "\n",
    "The choice of an embedding model depends on many factors:\n",
    "\n",
    "- The task, data modality, and data specifics;\n",
    "- The trade-off between search precision and resource usage (larger embeddings require more storage and memory);\n",
    "- The cost of inference (especially if you're using a third-party provider);\n",
    "etc\n",
    "\n",
    "---\n",
    "\n",
    "[FastEmbed](https://github.com/qdrant/fastembed) is an optimized embedding solution designed specifically for Qdrant. It delivers low-latency, CPU-friendly embedding generation, eliminating the need for heavy frameworks like PyTorch or TensorFlow. It uses quantized model weights and ONNX Runtime, making it significantly faster than traditional Sentence Transformers on CPU while maintaining competitive accuracy.\n",
    "\n",
    "FastEmbed supports:\n",
    "\n",
    "- Dense embeddings for text and images (the most common type in vector search, ones we're going to use today)\n",
    "- Sparse embeddings (e.g., BM25 and sparse neural embeddings)\n",
    "- Multivector embeddings (e.g., ColPali and ColBERT, late interaction models)\n",
    "- Rerankers\n",
    "\n",
    "All of these can be directly used in Qdrant (as Qdrant supports dense, sparse & multivectors along with hybrid search).\n",
    "FastEmbed’s integration with Qdrant allows you to directly pass text or images to the Qdrant client for embedding.\n",
    "\n",
    "### FastEmbed for Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n",
    "TextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b066078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an embedding model suitable for English text.\n",
    "# It also makes sense to select a unimodal model, since we’re not including images in our search\n",
    "# Also model that produces small-to-moderate-sized embeddings (e.g., 512 dimensions), so we don’t overuse resources in our simple setup. \n",
    "\n",
    "import json\n",
    "\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model[\"dim\"] == EMBEDDING_DIMENSIONALITY:\n",
    "        print(json.dumps(model, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c237f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jina-embeddings-v2-small-en has the best dim and match our preference\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee5197",
   "metadata": {},
   "source": [
    "Now we’re ready to configure and use Qdrant for semantic search. To fully understand what’s happening, here’s a quick overview of Qdrant’s core terminology:\n",
    "\n",
    "- Points are the central entity Qdrant works with. A point is a record consisting of an ID, a vector, and an optional payload.\n",
    "- A collection is a named set of points (i.e., vectors with optional payloads) that you can search within. Think of it as the container for your vector search solution, a single business problem solved.A collection is similar to index we used in elastic search ??\n",
    "\n",
    "Qdrant supports different types of vectors to enable different modes of data exploration and search (dense, sparse, multivectors, and named vectors). In this example, we’ll use the most common type, dense vectors.\n",
    "\n",
    "> Embeddings capture the semantic essence of the data, while the payload holds structured metadata.\n",
    "> This metadata becomes especially useful when applying filters or sorting during search. Qdrant's payloads can hold structured data like booleans, keywords, geo-locations, arrays, and nested objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b82c10",
   "metadata": {},
   "source": [
    "### Step 4: Create a Collection\n",
    "When creating a collection, we need to specify:\n",
    "\n",
    "- Name: A unique identifier for the collection.\n",
    "- Vector Configuration:\n",
    "  - Size: The dimensionality of the vectors.\n",
    "  - Distance Metric: The method used to measure similarity between vectors.\n",
    "\n",
    "There are additional parameters you can explore in our [documentationtext](https://qdrant.tech/documentation/concepts/collections/#create-a-collection)]. Moreover, you can configure other vector types in Qdrant beyond typical dense embeddings (e.g., for hybrid search). However, for this example, the simplest default configuration is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60d7f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'zoomcamp-rag' already exists\n"
     ]
    }
   ],
   "source": [
    "# Define the collection name\n",
    "collection_name = \"zoomcamp-rag\"\n",
    "\n",
    "# Check if collection exists\n",
    "if not client.collection_exists(collection_name):\n",
    "    # Create the collection if it doesn't exist\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
    "            distance=Distance.COSINE  # Distance metric for similarity search\n",
    "        )\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' created successfully\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23993b10",
   "metadata": {},
   "source": [
    "### Step 5: Create, Embed & Insert Points into the Collection\n",
    "\n",
    "[Points](https://qdrant.tech/documentation/concepts/points/#points) are the core data entities in Qdrant. Each point consists of:\n",
    "\n",
    "- ID. A unique identifier. Qdrant supports both 64-bit unsigned integers and UUIDs.\n",
    "- Vector. The embedding that represents the data point in vector space.\n",
    "- Payload (optional). Additional metadata as key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "id = 0\n",
    "\n",
    "for course in documents_raw:\n",
    "    for doc in course['documents']:\n",
    "\n",
    "        point = models.PointStruct(\n",
    "            id=id,\n",
    "            vector=models.Document(text=doc['text'], model=model_handle),\n",
    "            payload={\n",
    "                \"text\": doc['text'],\n",
    "                \"section\": doc['section'],\n",
    "                \"course\": course['course']\n",
    "            } #save all needed metadata fields\n",
    "        )\n",
    "        points.append(point)\n",
    "\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64649d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862c6bb3f75142b4bb69d9bc2978bd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15357cfe90e4edca894cfa950789fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8643a6c666f74477a7680a2de2e3d423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd1e00a85904a8c992ea53788fbedf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8527a809db45487592dce850a9b3d2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910ac6199df047eb8f2d3e1d66143f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed and upload points to our collection.\n",
    "# First, FastEmbed will fetch&download the selected model (path defaults to os.path.join(tempfile.gettempdir(), \"fastembed_cache\")), and perform inference directly on your machine.\n",
    "# Then, the generated points will be upserted into the collection, and the vector index will be built.\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c766dad",
   "metadata": {},
   "source": [
    "In addition to basic upsert, Qdrant supports batch upsert in both column- and record-oriented formats.\n",
    "\n",
    "The Python client offers:\n",
    "- Parallelization\n",
    "- Retries\n",
    "- Lazy batching\n",
    "These can be configured via parameters in the upload_collection and upload_points functions.\n",
    "For details, check the [documentation](https://qdrant.tech/documentation/concepts/points/#upload-points).\n",
    "\n",
    "### Study Data Visually\n",
    "Let’s explore the uploaded data in the Qdrant Web UI at http://localhost:6333/dashboard to study semantic similarity visually.\n",
    "\n",
    "For example, using the `Visualize tab` in the zoomcamp-rag collection, we can view all answers to the course questions (948 points) and see how they group together by meaning, additionally coloured by the course type.\n",
    "\n",
    "To do that, run the following command:\n",
    "\n",
    "```bash\n",
    "{\n",
    "  \"limit\": 948,\n",
    "  \"color_by\": {\n",
    "    \"payload\": \"course\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This 2D representation is the result of dimensionality reduction applied to jina-embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1ceca",
   "metadata": {},
   "source": [
    "### Step 6: Running a Similarity Search\n",
    "#### How Similarity Search Works in Quadrant\n",
    "\n",
    "- Qdrant compares the query vector to stored vectors (based on a vector index) using the distance metric defined when creating the collection.\n",
    "- The closest matches are returned, ranked by similarity.\n",
    "\n",
    "Vector index is built for approximate nearest neighbor (ANN) search, making large-scale vector search feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba72b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, limit=1):\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b7fe7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey\\u2019s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)\",\n",
      "  \"section\": \"3. Machine Learning for Classification\",\n",
      "  \"question\": \"How to calculate Root Mean Squared Error?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# let's pick a random to ask. NB. we didnt upload the questions, only responses.\n",
    "import random\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course['documents'])\n",
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa3dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b9ca7452014ac6ab9a023ae0719809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f8e68dc757469897cc28b786a28d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36e524660c24147965a73a1f141b340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a096a0fe7760475dafa7d5a84f15d898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4c036fef5b4d12a96f10cfa42a2bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67064e6d2c845f3861040f79771f597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=552, version=1, score=0.8832736, payload={'text': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)', 'section': '3. Machine Learning for Classification', 'course': 'machine-learning-zoomcamp'}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = search(course_piece['question'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ddeda",
   "metadata": {},
   "source": [
    "`score` – the cosine similarity between the `question` and `text` embeddings.\n",
    "\n",
    "Let’s compare the original and retrieved answers for our randomly selected question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3816a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "How to calculate Root Mean Squared Error?\n",
      "\n",
      "Top Retrieved Answer:\n",
      "We can use sklearn & numpy packages to calculate Root Mean Squared Error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "Rmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\n",
      "Added by Radikal Lukafiardi\n",
      "You can also refer to Alexey’s notebook for Week 2:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\n",
      "which includes the following code:\n",
      "def rmse(y, y_pred):\n",
      "error = y_pred - y\n",
      "mse = (error ** 2).mean()\n",
      "return np.sqrt(mse)\n",
      "(added by Rileen Sinha)\n",
      "\n",
      "Original Answer:\n",
      "We can use sklearn & numpy packages to calculate Root Mean Squared Error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "Rmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\n",
      "Added by Radikal Lukafiardi\n",
      "You can also refer to Alexey’s notebook for Week 2:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\n",
      "which includes the following code:\n",
      "def rmse(y, y_pred):\n",
      "error = y_pred - y\n",
      "mse = (error ** 2).mean()\n",
      "return np.sqrt(mse)\n",
      "(added by Rileen Sinha)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question:\\n{course_piece['question']}\\n\")\n",
    "print(\"Top Retrieved Answer:\\n{}\\n\".format(result.points[0].payload['text']))\n",
    "print(\"Original Answer:\\n{}\".format(course_piece['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa7c4fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\n",
      "Older news:[source1] [source2]\n"
     ]
    }
   ],
   "source": [
    "print(search(\"What if I submit homeworks late?\").points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60650066",
   "metadata": {},
   "source": [
    "### Step 7: Running a Similarity Search with Filters\n",
    "We can refine our search using metadata filters. This is ismilar to our toy search engine where we can set filters on programmes.\n",
    "\n",
    "> Qdrant’s custom vector index implementation, Filterable HNSW, allows for precise and scalable vector search with filtering conditions.\n",
    "\n",
    "For example, we can search for an answer to a question related to a specific course from the three available in the dataset.\n",
    "Using a `must` filter ensures that all specified conditions are met for a data point to be included in the search results.\n",
    "\n",
    "> Qdrant also supports other filter types such as `should`, `must_not`, `range`, and more. For a full overview, check the [Filtering Guide](https://qdrant.tech/articles/vector-search-filtering/)\n",
    "\n",
    "To enable efficient filtering, we need to turn on [indexing of payload fields](https://qdrant.tech/documentation/concepts/indexing/#payload-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31efc140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=3, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"course\",\n",
    "    field_schema=\"keyword\" # exact matching on string metadata fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e740fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our searh function to allow filtering\n",
    "def search_in_course(query, course=\"mlops-zoomcamp\", limit=1):\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=model_handle\n",
    "        ),\n",
    "        query_filter=models.Filter( # this allow us to filter by course name\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"course\",\n",
    "                    match=models.MatchValue(value=course)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa1a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "them = [\"data-engineering-zoomcamp\", \"machine-learning-zoomcamp\", \"mlops-zoomcamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f82f9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from data-engineering-zoomcamp:\n",
      "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\n",
      "Older news:[source1] [source2]\n",
      "\n",
      "Results from machine-learning-zoomcamp:\n",
      "Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\n",
      "(Added by Rileen Sinha, based on answer by Alexey on Slack)\n",
      "\n",
      "Results from mlops-zoomcamp:\n",
      "Please choose the closest one to your answer. Also do not post your answer in the course slack channel.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let’s see how the same question is answered across different courses:\n",
    "for c in them:\n",
    "    print(f\"Results from {c}:\")\n",
    "    print(search_in_course(\"What if I submit homeworks late?\", c).points[0].payload['text'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631229f",
   "metadata": {},
   "source": [
    "It seems mlops doesnt have related responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2ddd6",
   "metadata": {},
   "source": [
    "### Further References\n",
    "\n",
    "- [Vector Search Manual](https://qdrant.tech/articles/vector-search-manuals/): To dive deeper into efficient vector search setup (data prep, organization, and storage in a production-ready vector search solution)\n",
    "- [Hybrid Search](https://qdrant.tech/articles/hybrid-search/): It combines the strengths of both keywords-based search and vector search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
