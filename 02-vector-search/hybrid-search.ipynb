{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd9441f",
   "metadata": {},
   "source": [
    "### Hybrid Search with Qdrant\n",
    "\n",
    "- Vector search: A paradigm shift from traditional keyword-based search, enabling semantic understanding of data.\n",
    "- Embeddings: Numerical representations of data (text, images, etc.) in high-dimensional space (e.g., 384 or 1536 dimensions).\n",
    "    - They are generated by ML models (e.g., OpenAI's text-embedding-ada-002, BERT, or Sentence Transformers like \"all-MiniLM-L6-v2\").\n",
    "    - Similar items cluster nearby in vector space.\n",
    "- Vector Indexing: Used to optimize fast similarity search in high-dimensional spaces. It uses two approaches to indexing\n",
    "    - Flat Indexing (Brute-force) – Exact but slow (O(n) complexity).\n",
    "    - Approximate Nearest Neighbors (ANN) which trades accuracy for speed. \n",
    "        - HNSW (Hierarchical Navigable Small World): Graph-based, fast and accurate.*\n",
    "        - IVF (Inverted File Index): Clustering-based, good for large datasets.\n",
    "        - PQ (Product Quantization): Compresses vectors for memory efficiency.\n",
    "- Similarity Metrics\n",
    "    - Cosine Similarity: Measures angle between vectors (ignores magnitude). Best for text.\n",
    "    - Euclidean Distance: Straight-line distance in vector space. Good for images.\n",
    "    - Dot Product: Combines magnitude and direction. Used when vector norms matter.\n",
    "- Retrieval Process: Query → Embedding → Nearest Neighbor Search → Ranked Results.\n",
    "\n",
    "\n",
    "#### Hybrid search\n",
    "- Combines vector search with traditional keyword search (BM25, TF-IDF) for better results. In this case, two things happen:\n",
    "    - Vector search handles semantic meaning (\"synonyms, paraphrases\").\n",
    "    - Keyword search handles exact matches (\"product codes, names\").\n",
    "- Implementation of hybrid search\n",
    "    - Reciprocal Rank Fusion (RRF): Merges rankings from both methods.\n",
    "    - Weighted Scores: Assign weights (e.g., 0.7 vector + 0.3 keyword).\n",
    "\n",
    "#### Our Implementation\n",
    "For our context, we are using vector search and RAG to improve response. \n",
    "- Vector Search: Retrieves relevant context for LLMs.\n",
    "- The workflow:\n",
    "    - User query → embedded → vector search over knowledge base → Top-k results fed to LLM as context → LLM generates answer grounded in retrieved docs.\n",
    "- There is always room for improvement to improve accuracy (a slower or more accurate model like cross-encoder) and using static embedding (against dynamic embeddings)\n",
    "\n",
    "\n",
    "#### End-to-End Vector Search Pipeline\n",
    "**Data Preparation**\n",
    "- Chunk documents → Generate embeddings → Store in vector DB (Qdrant)\n",
    "\n",
    "**Query Processing**\n",
    "- Embed query → Search index → Hybrid ranking (optional) (Qdrant or elastic search)\n",
    "\n",
    "**Result Delivery**\n",
    "- Return nearest neighbors → Optionally rerank → Pass to LLM (RAG) (Gemini, OpenAI, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7bcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Search finds results matching meaning (not just keywords).\n",
    "# Vector Search  searches using numerical vectors (embeddings) instead of keywords.\n",
    "# Hybrid Search combines vector search (semantic) + keyword search (exact matches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f75eb",
   "metadata": {},
   "source": [
    "- Sparse Vectors are high-dimensional vectors where most values are zero (e.g., TF-IDF, BM25).\n",
    "- Dense Vectors: Sparse = keyword-based; Dense = ML-generated (embeddings).\n",
    "\n",
    "### When to Use Which?\n",
    "- Use Vector Search when you need semantic similarity (e.g., RAG, recommendations) or the data is unstructured (text, images).\n",
    "- Use Semantic Search when hybrid logic (keywords + vectors + business rules) is needed or one requires explainability (e.g., search engines showing why a result matched)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da71032",
   "metadata": {},
   "source": [
    "### Sparse vectors\n",
    "Surprisingly, keyword-based search is also implemented as vector search, but these vectors are usually sparse. That means the majority of the dimensions of such a vector are just zeros. A non-zero value at a particular vector dimension indicates the presence of a term from the dictionary assigned to that position. In other words, in sparse vectors, we have a dictionary in which each word/phrase gets its unique position. Since vectors are sparse, the dictionary can theoretically grow indefinitely, as we can append a new term at the very end.\n",
    "\n",
    "The fact of using a flexible dictionary, make the sparse vectors excel in exact matches, as they can cover texts that would be sets of random characters for the dense vectors - such as proper names or identifiers. Dense embedding models also have a dictionary, but once the model is trained, extending them is not that easy, and requires fine-tuning of the model. A typical user rarely goes that far.\n",
    "\n",
    "### BM25\n",
    "There are plenty of different options for creating sparse embeddings, but BM25 is an industry standard, and its most popular form comes from the 90s. It's a statistical model (no neural networks involved), which makes it really fast and lightweight. It's usually a solid baseline in search benchmarks so you should not ignore it.\n",
    "\n",
    "BM25 stands for Best Matching 25, and it was just the 25th attempt to create a formula that calculates how relevant a particular document is, given a query. In general, BM25 is a ranking function that helps search engines determine how relevant a document is to a query by combining two key concepts: Term Frequency (TF) and Inverse Document Frequency (IDF). BM25 also incorporates document length normalization to prevent longer documents from having an unfair advantage simply due to their size.\n",
    "\n",
    "- The Term Frequency component rewards documents that contain the query terms multiple times, but with diminishing returns - so a document with 10 occurrences of a word isn't necessarily 10 times better than one with just 1 occurrence.\n",
    "- The Inverse Document Frequency part boosts the importance of rare words while reducing the weight of common words that appear in many documents, since rare terms are typically more informative for distinguishing relevant results.\n",
    "\n",
    "In our case with Qdrant, we'll use an implementation available in FastEmbed. Let's start with the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681f485",
   "metadata": {},
   "source": [
    "### Step 1: Connect to Qdrant\n",
    "\n",
    "We already setup Qdrant. Just start docker and we are up\n",
    "- ```docker ps -a```\n",
    "- ```docker start ID```\n",
    "- launch the forwarded port and add ```/dashboard``` at the end to get into the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad93af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='zoomcamp-rag'), CollectionDescription(name='zoomcamp-faq')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are already setup considering the Qdrant server is already up initially\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d360671",
   "metadata": {},
   "source": [
    "### Step 2: Sparse vector search with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6539356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df26901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7cebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c7d290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to create a collection first. Qdrant will handle the IDF calculations, if we configure it to. That's required for BM25, otherwise it won't boost the rare words.\n",
    "\n",
    "# Create the collection with specified sparse vector parameters\n",
    "client.create_collection(\n",
    "    collection_name=\"zoomcamp-sparse\",\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2961aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61390fca21a74ce095455672646acd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44e1d28cae4484fa2f96674bc457a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ee42896cbe4afca559220526258aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "english.txt:   0%|          | 0.00/936 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d153f3126040c5a2353c49a3a3ba7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "finnish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3daf3066a9248b79f7f939edbd26cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dutch.txt:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e068485aaf1141bd988fc20e63b3302f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "arabic.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c328cee4ff429fabc7661781a40077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "german.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36930d28bacf41e6bb6c056e60c8243e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "french.txt:   0%|          | 0.00/813 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021478a25a2c409983e6b4ed55c70082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "danish.txt:   0%|          | 0.00/424 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0dde9a4f4e431c9a6534ddfeefe216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "greek.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0054e9d5a5d4117a42acd04b4761db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hungarian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3e481c9fbb4b058992958008e435a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "portuguese.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facb12f9870b4ef9939ade75460219cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "norwegian.txt:   0%|          | 0.00/851 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42be09a7880549bbbf4fc47d031374cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "italian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eee9fac4afc403d8350a55a6be54678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "romanian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2599a593ec974aa7a016eac7e5b04070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "russian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111db8fe6aad46b0b4b79c081e2af4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spanish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0934cdf4c64054aa3b1bda2e4bc1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "swedish.txt:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e407debd6f0d4c5da9c5dbc81708e31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "turkish.txt:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastEmbed comes with a BM25 implementation that we can use as any other model.\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Send the points to the collection\n",
    "client.upsert(\n",
    "    collection_name=\"zoomcamp-sparse\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"], \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"],\n",
    "            }\n",
    "        )\n",
    "        for course in documents_raw\n",
    "        for doc in course[\"documents\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ad6d6",
   "metadata": {},
   "source": [
    "The upload operation was fast because BM25 is only a statistical model and does not require a neural network, so it is fast compared to dense embedding models.\n",
    "\n",
    "### Step 3: Running sparse vector search with BM25\n",
    "Right now, our vectors are ready to be searched over. Let's create a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0fa3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse\",\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\",\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1dc83ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search(\"Qdrant\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5039b49",
   "metadata": {},
   "source": [
    "Sparse vectors can return no results, if none of the keywords from the query were ever used in the documents. No matter if there are some synonyms. Terminology does matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8821f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use round() function or f-strings\n",
      "round(number, 4)  - this will round number up to 4 decimal places\n",
      "print(f'Average mark for the Homework is {avg:.3f}') - using F string\n",
      "Also there is pandas.Series. round idf you need to round values in the whole Series\n",
      "Please check the documentation\n",
      "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\n",
      "Added by Olga Rudakova\n"
     ]
    }
   ],
   "source": [
    "results = search(\"pandas\")\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f8a22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0392046"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scores returned by BM25 are not calculated with cosine similarity, but with BM25 formula. They are not bounded to a specific range, but are virtually unbounded. \n",
    "# Let's see how they may look like. That's an important observation before we start implementing hybrid search.\n",
    "\n",
    "results[0].score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6fd20",
   "metadata": {},
   "source": [
    "#### Natural language like queries\n",
    "Let's try again with a random question from our dataset to see how well sparse vector search can work with longer, natural language like queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fca100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N\",\n",
      "  \"section\": \"4. Evaluation Metrics for Classification\",\n",
      "  \"question\": \"How to get all classification metrics?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(202508)\n",
    "\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course[\"documents\"])\n",
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e623c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\n",
      "Use classification_report from sklearn. For more info check here.\n",
      "Abhishek N\n"
     ]
    }
   ],
   "source": [
    "results = search(course_piece[\"question\"])\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef002d9",
   "metadata": {},
   "source": [
    "### Step 4: Qdrant Universal Query API - prefetching\n",
    "Qdrant's `.query_points` method allows building multi-step search pipelines which can incorporate various methods into a single call. For example, we can retrieve some candidates with dense vector search, and then rerank them with sparse search, or use a fast method for initial retrieval and precise, but slow, reranking.\n",
    "\n",
    "Let's create another collection that will keep both dense and sparse representations. Qdrant named vectors allow us to store multiple representations per point and it proves useful especially when we want to use mulitple models in our applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e1fa97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the collection with both vector types\n",
    "client.create_collection(\n",
    "    collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "    vectors_config={\n",
    "        # Named dense vector for jinaai/jina-embeddings-v2-small-en\n",
    "        \"jina-small\": models.VectorParams(\n",
    "            size=512,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43af2e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e069b07dd1a749b89a0a2f6732f406b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13245b37b522482f9de6c45a71a29415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e11b89c4bb2492581f575d82289ac1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b262ea11dff4d26851ffdf3f8f71225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6f7efa0f4c45dfa12d68e90a7db0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f83955850a64fbda092304a4fa99d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to upload all the vectors into the newly created collection.\n",
    "client.upsert(\n",
    "    collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"jina-small\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"], \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"],\n",
    "            }\n",
    "        )\n",
    "        for course in documents_raw\n",
    "        for doc in course[\"documents\"]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90d47d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_stage_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                # Prefetch ten times more results, then\n",
    "                # expected to return, so we can really rerank\n",
    "                limit=(10 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\", \n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "975aa25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N\",\n",
      "  \"section\": \"4. Evaluation Metrics for Classification\",\n",
      "  \"question\": \"How to get all classification metrics?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f75a6063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\n",
      "Use classification_report from sklearn. For more info check here.\n",
      "Abhishek N\n"
     ]
    }
   ],
   "source": [
    "results = multi_stage_search(course_piece[\"question\"])\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45c4ff",
   "metadata": {},
   "source": [
    "### Step 5: Building Hybrid Search\n",
    "In real production systems, you don't need to choose just one vector type. You never know what kind of queries your users will send to the system. E-commerce search might be just fine with lexical search on top of sparse vectors, as people will tend to send keywords, but in conversational systems, such as chatbots, natural language questions might be more frequent. Using one model as a retriever and another one as reranker is not the only way of how to use dense and sparse in a single system.\n",
    "\n",
    "Hybrid Search is a technique for combining results coming from different search methods - for example dense and sparse. There isn't a clear definition of how exactly to implement it, as the main problem is how to mix results coming from methods which are incompatible. Dense and sparse search scores can't be compared directly, so we need another method that will order the final results somehow.\n",
    "\n",
    "There are two terms important for Hybrid Search: fusion and reranking.\n",
    "\n",
    "#### Fusion\n",
    "Fusion is a set of methods which work on the scores/ranking as returned by the individual methods. There are various ways of how to achieve that, but Reciprocal Rank Fusion is the most popular technique. It is based on the rankings of the documents in each methods used, and these rankings are used to calculate the final scores. You will never calculate these scores, as Qdrant has some built-in capabilities that we will use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9991833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        # Fusion query enables fusion on the prefetched results\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b224e1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N\",\n",
      "  \"section\": \"4. Evaluation Metrics for Classification\",\n",
      "  \"question\": \"How to get all classification metrics?\"\n",
      "}\n",
      "How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\n",
      "Use classification_report from sklearn. For more info check here.\n",
      "Abhishek N\n"
     ]
    }
   ],
   "source": [
    "results = rrf_search(course_piece[\"question\"])\n",
    "print(json.dumps(course_piece, indent=2))\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b559754",
   "metadata": {},
   "source": [
    "#### Reranking\n",
    "Reranking is a broader term related to Hybrid Search. Fusion is one of the ways to rerank the results of multiple methods, but you can also apply a slower method that won't be effective enough to search over all the documents. But there is more to it. Business rules are often important for retrieval, as you prefer to show documents coming from the most recent news, for instance.\n",
    "\n",
    "### Next steps\n",
    "Dense and sparse vector search methods might not be enough in some cases, but both are fast enough to be used as initial retrievers. Plenty of more accurate yet slower methods exist, such as cross-encoders or multivector representations. These topics are definitely more advanced, and we won't cover them right now.\n",
    "\n",
    "```The entire topics covered now (hybrid search especially) is useful especially for elearning systems where students ask questions to a knowledgebase of their class materials and the responses are sent to an LLM to make sense of the responses. This way the search system is more precise and relevant and the LLM is able to get more focused context.```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
